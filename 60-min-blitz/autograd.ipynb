{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5eff36e-835e-4527-ba7e-c1f83ff5f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33607488-1911-4f8a-95c4-47bea82dcbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is implemented on CPU\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "\n",
    "# Forward Pass\n",
    "prediction = model(data)\n",
    "\n",
    "# Backward pass: Autograd then calculates and stores the gradients for each model parameter in the parameter’s .grad attribute.\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass\n",
    "\n",
    "# Load optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# Initiate gradient descent, the optimizer adjusts each parameter by its gradient stored in .grad\n",
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1196757a-7e31-4fb1-80ea-b18bb7efde28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# Differentiation in Autograd\n",
    "# Let’s assume a and b to be parameters of an NN, and Q to be the error\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "# We need to explicitly pass a gradient argument in Q.backward() because it is a vector.\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# When we call .backward() on Q, autograd calculates these gradients and stores them in the respective tensors’ .grad attribute.\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d418cac0-d8f2-4592-ab6b-688f8dd40173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "# Exclusion from DAG\n",
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "# parameters that don’t compute gradients are usually called frozen parameters\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da2d516-48fb-478b-8fd0-41319493d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. \n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# In resnet, the classifier is the last linear layer model.fc. (fully connected layer)\n",
    "# We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "# Now all parameters in the model, except the parameters of model.fc, are frozen.\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7322cf1-044d-4290-82c1-26a66e538dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_ratio_torch",
   "language": "python",
   "name": "financial_ratio_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
